from fastapi import FastAPI, HTTPException, Depends, status
from fastapi import APIRouter
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, HttpUrl
from typing import List, Optional
import requests
import tempfile
import os
from pathlib import Path
import logging
from contextlib import asynccontextmanager
import hashlib
import json
from datetime import datetime
import re

# PDF processing
from langchain.document_loaders import PyPDFLoader
import tempfile

# Vector database and embeddings
import pinecone
from pinecone import Pinecone, ServerlessSpec
import numpy as np

# Text processing
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# OpenAI integration
import openai
from openai import OpenAI

# PostgreSQL and SQLAlchemy
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.dialects.postgresql import UUID, ARRAY
from pgvector.sqlalchemy import Vector
import uuid

import logging
from typing import List

# Environment variables
from dotenv import load_dotenv
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variables for models and clients
embedding_model = None
pinecone_client = None
pinecone_index = None
openai_client = None
db_engine = None
SessionLocal = None

# SQLAlchemy setup
Base = declarative_base()


def log_document_content(content: str, max_chars: int = 1000):
    """Log first part of document content for debugging"""
    logger.info(f"ðŸ“„ Document content preview ({len(content)} total chars):")
    logger.info(f"First {max_chars} characters:")
    logger.info("-" * 50)
    logger.info(content[:max_chars])
    logger.info("-" * 50)

def log_chunks_preview(chunks: List[str], max_chunks: int = 3):
    """Log preview of created chunks"""
    logger.info(f"ðŸ“¦ Created {len(chunks)} chunks. Preview of first {max_chunks}:")
    for i, chunk in enumerate(chunks[:max_chunks]):
        logger.info(f"Chunk {i+1} ({len(chunk)} chars): {chunk[:200]}...")

def log_search_results(question: str, chunks: List[str], max_results: int = 2):
    """Log search results for debugging"""
    logger.info(f"ðŸ” Search results for: '{question[:50]}...'")
    logger.info(f"Found {len(chunks)} relevant chunks:")
    for i, chunk in enumerate(chunks[:max_results]):
        logger.info(f"Result {i+1}: {chunk[:150]}...")

# Database Models
class Document(Base):
    """Store document metadata and processing status"""
    __tablename__ = "documents"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    url = Column(String, unique=True, index=True, nullable=False)
    url_hash = Column(String, unique=True, index=True, nullable=False)
    title = Column(String, nullable=True)
    content = Column(Text, nullable=False)
    processed_at = Column(DateTime, default=datetime.utcnow)
    chunk_count = Column(Integer, default=0)
    is_active = Column(Boolean, default=True)

class DocumentChunk(Base):
    """Store document chunks with embeddings"""
    __tablename__ = "document_chunks"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), index=True, nullable=False)
    chunk_index = Column(Integer, nullable=False)
    content = Column(Text, nullable=False)
    embedding = Column(Vector(384))  # BAAI/bge-large-en-v1.5 produces 1024-dim vectors
    created_at = Column(DateTime, default=datetime.utcnow)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize resources on startup and cleanup on shutdown"""
    global embedding_model, pinecone_client, pinecone_index, openai_client, db_engine, SessionLocal
    
    try:
        # Initialize database
        logger.info("Connecting to PostgreSQL database...")
        DATABASE_URL = os.getenv("DATABASE_URL")
        if not DATABASE_URL:
            raise ValueError("DATABASE_URL not found in environment variables")
        
        db_engine = create_engine(DATABASE_URL)
        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=db_engine)
        
        # Create tables
        Base.metadata.create_all(bind=db_engine)
        logger.info("Database tables created/verified")
        
        # Initialize embedding model
        logger.info("Loading embedding model...")
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
       
        # Initialize Pinecone client
        logger.info("Initializing Pinecone...")
        pinecone_client = Pinecone(
            api_key=os.getenv("PINECONE_API_KEY"),
            environment=os.getenv("PINECONE_ENVIRONMENT")
        )
        
        # Create or connect to index
        index_name = os.getenv("PINECONE_INDEX")
        pinecone_index = pinecone_client.Index(index_name)
        
        # Initialize OpenAI client
        logger.info("Initializing OpenAI GPT-4o...")
        openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        logger.info("All services initialized successfully")
        yield
        
    except Exception as e:
        logger.error(f"Failed to initialize services: {e}")
        raise
    finally:
        logger.info("Shutting down services")

# Initialize FastAPI app with lifespan
app = FastAPI(
    title="HackRx RAG API with Enhanced Retrieval",
    description="Enhanced RAG system with improved retrieval for insurance policy document processing",
    version="2.1.0",
    lifespan=lifespan
)

# Security
security = HTTPBearer()

# Pydantic models
class ProcessRequest(BaseModel):
    documents: HttpUrl
    questions: List[str]

class ProcessResponse(BaseModel):
    answers: List[str]

VALID_TOKEN = os.getenv("BEARER_TOKEN")

if not VALID_TOKEN:
    raise ValueError("BEARER_TOKEN is not set in the environment. Please set it before running the app.")

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify Bearer token"""
    if credentials.credentials != VALID_TOKEN:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return credentials

def get_db():
    """Get database session"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_url_hash(url: str) -> str:
    """Generate hash for URL to use as unique identifier"""
    return hashlib.sha256(url.encode()).hexdigest()

class DatabaseManager:
    """Handle database operations for document caching"""
    
    @staticmethod
    def get_document_by_url(db: Session, url: str) -> Optional[Document]:
        """Check if document already exists in database"""
        url_hash = get_url_hash(url)
        return db.query(Document).filter(Document.url_hash == url_hash, Document.is_active == True).first()
    
    @staticmethod
    def save_document(db: Session, url: str, content: str, chunks: List[str]) -> Document:
        """Save document and its chunks to database"""
        try:
            url_hash = get_url_hash(url)
            
            # Create document record
            document = Document(
                url=url,
                url_hash=url_hash,
                content=content,
                chunk_count=len(chunks)
            )
            db.add(document)
            db.flush()  # Get the document ID
            
            # Generate embeddings for chunks
            embeddings = embedding_model.encode(chunks)
            
            # Save chunks with embeddings
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_record = DocumentChunk(
                    document_id=document.id,
                    chunk_index=i,
                    content=chunk,
                    embedding=embedding.tolist()
                )
                db.add(chunk_record)
            
            db.commit()
            logger.info(f"Saved document with {len(chunks)} chunks to database")
            return document
            
        except Exception as e:
            db.rollback()
            logger.error(f"Failed to save document to database: {e}")
            raise
    
    @staticmethod
    def get_document_chunks(db: Session, document_id: uuid.UUID) -> List[DocumentChunk]:
        """Get all chunks for a document"""
        return db.query(DocumentChunk).filter(DocumentChunk.document_id == document_id).all()
    
    @staticmethod
    def search_similar_chunks(db: Session, query_embedding: List[float], limit: int = 15) -> List[DocumentChunk]:
        """Search for similar chunks using vector similarity with more results"""
        try:
            # Use pgvector's cosine similarity with increased limit
            chunks = db.query(DocumentChunk).order_by(
                DocumentChunk.embedding.cosine_distance(query_embedding)
            ).limit(limit).all()
            
            return chunks
        except Exception as e:
            logger.error(f"Failed to search similar chunks: {e}")
            return []

class PDFProcessor:
    """Handle PDF download and text extraction using LangChain"""
    
    @staticmethod
    def download_and_extract_pdf(url: str) -> str:
        """Download PDF from URL and extract text using LangChain PyPDFLoader"""
        try:
            # Create temporary file for PDF
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                # Download PDF
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=60)  # Increased timeout
                response.raise_for_status()
                
                if 'application/pdf' not in response.headers.get('content-type', ''):
                    logger.warning(f"Content type is not PDF: {response.headers.get('content-type')}")
                
                # Write PDF content to temp file
                temp_file.write(response.content)
                temp_file_path = temp_file.name
            
            try:
                # Use LangChain PyPDFLoader to extract text
                loader = PyPDFLoader(temp_file_path)
                pages = loader.load()
                
                # Combine all pages with better formatting
                text = ""
                for i, page in enumerate(pages):
                    page_content = page.page_content.strip()
                    if page_content:  # Only add non-empty pages
                        text += f"\n=== Page {i + 1} ===\n{page_content}\n"
                
                if not text.strip():
                    raise ValueError("No text could be extracted from the PDF")
                
                logger.info(f"Extracted {len(text)} characters from {len(pages)} pages")
                return text.strip()
            
            finally:
                # Clean up temporary file
                try:
                    os.unlink(temp_file_path)
                except:
                    pass
            
        except Exception as e:
            logger.error(f"Failed to download and extract PDF: {e}")
            raise HTTPException(status_code=400, detail=f"Failed to process PDF: {str(e)}")

class ImprovedTextChunker:
    """Enhanced text chunking with better strategies for legal documents"""
    
    def __init__(self, chunk_size: int = 1200, overlap: int = 200):  # INCREASED sizes
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            length_function=len,
            separators=[
                "\n\n",           # Paragraph breaks
                "\nCLAUSE",       # Policy clauses
                "\nSECTION",      # Policy sections  
                "\nCOVERAGE",     # Coverage sections
                "\nEXCLUSION",    # Exclusions
                "\nDEFINITION",   # Definitions
                "\n",             # Line breaks
                ". ",             # Sentence breaks
                " ",              # Word breaks
                ""                # Character breaks
            ]
        )
    
    def chunk_text(self, text: str) -> List[str]:
        """Split text into overlapping chunks with improved preprocessing"""
        try:
            # Clean and preprocess text
            cleaned_text = self.preprocess_text(text)
            
            # Split into chunks
            chunks = self.text_splitter.split_text(cleaned_text)
            
            # Post-process chunks
            processed_chunks = []
            for chunk in chunks:
                processed_chunk = self.postprocess_chunk(chunk)
                if len(processed_chunk.strip()) > 100:  # Only keep substantial chunks
                    processed_chunks.append(processed_chunk)
            
            logger.info(f"Created {len(processed_chunks)} processed chunks from text")
            return processed_chunks
            
        except Exception as e:
            logger.error(f"Failed to chunk text: {e}")
            return []
    
    def preprocess_text(self, text: str) -> str:
        """Enhanced preprocessing for insurance documents"""
        # Fix common insurance document issues
        text = re.sub(r'\n\s*CLAUSE\s+(\d+)', r'\n\nCLAUSE \1', text)
        text = re.sub(r'\n\s*SECTION\s+(\d+)', r'\n\nSECTION \1', text)
        text = re.sub(r'\n\s*EXCLUSION\s*:', r'\n\nEXCLUSION:', text)
        
        # Remove excessive whitespace but preserve structure
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        return text.strip()
    
    def postprocess_chunk(self, chunk: str) -> str:
        """Clean up individual chunks"""
        # Remove page markers at the start/end of chunks
        chunk = re.sub(r'^=== Page \d+ ===\s*', '', chunk)
        chunk = re.sub(r'\s*=== Page \d+ ===$', '', chunk)
        
        # Clean up whitespace
        chunk = re.sub(r'\s+', ' ', chunk)
        chunk = chunk.strip()
        
        return chunk

class EnhancedHybridVectorStore:
    """Enhanced hybrid vector storage with improved search strategies"""
    
    def __init__(self):
        self.namespace = "insurance_docs"
    
    def search_postgresql_enhanced(self, db: Session, query: str, limit: int = 15) -> List[str]:
        """Enhanced PostgreSQL search with query expansion"""
        try:
            # Generate embeddings for original query
            original_embedding = embedding_model.encode([query])[0].tolist()
            
            # Get similar chunks
            chunks = DatabaseManager.search_similar_chunks(db, original_embedding, limit)
            
            # Extract content and log similarity scores for debugging
            results = []
            for chunk in chunks:
                results.append(chunk.content)
            
            logger.info(f"PostgreSQL search found {len(results)} chunks for query: '{query[:50]}...'")
            return results
            
        except Exception as e:
            logger.error(f"PostgreSQL search failed: {e}")
            return []
    
    def search_pinecone_enhanced(self, query: str, limit: int = 10) -> List[str]:
        """Enhanced Pinecone search with multiple query strategies"""
        try:
            query_embedding = embedding_model.encode([query])[0].tolist()
            
            results = pinecone_index.query(
                vector=query_embedding,
                top_k=limit,
                namespace=self.namespace,
                include_metadata=True
            )
            
            documents = []
            for match in results.matches:
                if 'text' in match.metadata:
                    documents.append(match.metadata['text'])
            
            logger.info(f"Pinecone search found {len(documents)} chunks")
            return documents
            
        except Exception as e:
            logger.error(f"Pinecone search failed: {e}")
            return []
    
    def multi_query_search(self, db: Session, original_query: str) -> List[str]:
        """Search using multiple query variations for better coverage"""
        all_results = set()
        
        # Original query
        results1 = self.search_postgresql_enhanced(db, original_query, limit=10)
        all_results.update(results1)
        
        # Query variations for better coverage
        query_variations = self.generate_query_variations(original_query)
        
        for variation in query_variations[:2]:  # Limit to 2 variations to avoid too many results
            results = self.search_postgresql_enhanced(db, variation, limit=5)
            all_results.update(results)
        
        # Convert back to list and limit
        final_results = list(all_results)[:15]  # Increased limit
        logger.info(f"Multi-query search found {len(final_results)} unique chunks")
        
        return final_results
    
    def generate_query_variations(self, query: str) -> List[str]:
        """Generate query variations for better retrieval"""
        variations = []
        
        # Extract key terms
        key_terms = self.extract_key_terms(query)
        
        if key_terms:
            # Create variations with key terms
            variations.append(" ".join(key_terms))
            
            # Create individual key term queries
            for term in key_terms[:2]:  # Limit to top 2 key terms
                variations.append(term)
        
        return variations
    
    def extract_key_terms(self, query: str) -> List[str]:
        """Extract key terms from query"""
        # Simple keyword extraction
        import re
        
        # Remove common stop words
        stop_words = {'what', 'is', 'the', 'how', 'does', 'are', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        
        # Extract words (keep important terms like Article, Constitution, etc.)
        words = re.findall(r'\b[A-Za-z]+\b', query.lower())
        key_terms = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Prioritize legal/constitutional terms
        priority_terms = ['constitution', 'article', 'amendment', 'rights', 'fundamental', 'directive', 'principles', 'president', 'supreme', 'court', 'parliament', 'state', 'emergency']
        
        prioritized = []
        for term in priority_terms:
            if term in key_terms:
                prioritized.append(term)
        
        # Add remaining terms
        for term in key_terms:
            if term not in prioritized:
                prioritized.append(term)
        
        return prioritized[:5]  # Return top 5 key terms
        
    def is_generic_chunk(self, content: str) -> bool:
        """Filter out generic/useless chunks"""
        generic_phrases = [
            "covered provided the Policy has been continuously renewed",
            "WE/OUR/US/COMPANY means UNITED INDIA INSURANCE",
            "without any break. 57.",
            "the time, that is, which is to four right",
            "school. One day, however, the boy immediately"
        ]
        
        # If chunk is mostly generic phrases, skip it
        for phrase in generic_phrases:
            if phrase.lower() in content.lower():
                return True
        
        # Skip very short chunks
        if len(content.strip()) < 100:
            return True
            
        return False
    
    def hybrid_search_enhanced(self, db: Session, query: str) -> List[str]:
        """FIXED: Better search with more relevant chunks"""
        try:
            # Get MORE relevant chunks per query
            original_embedding = embedding_model.encode([query])[0].tolist()
            
            # INCREASED SEARCH RESULTS
            chunks = DatabaseManager.search_similar_chunks(db, original_embedding, limit=25)  # Increased from 15
            
            # Extract content and filter out generic chunks
            results = []
            for chunk in chunks:
                content = chunk.content.strip()
                
                # FILTER OUT GENERIC/IRRELEVANT CHUNKS
                if len(content) > 50 and not self.is_generic_chunk(content):
                    results.append(content)
            
            logger.info(f"Enhanced search found {len(results)} quality chunks")
            return results[:20]  # Return top 20 quality chunks
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []
    
    def add_to_pinecone_fallback(self, chunks: List[str]):
        """Add chunks to Pinecone as fallback"""
        try:
            embeddings = embedding_model.encode(chunks)
            batch_size = 20
            
            for batch_idx in range(0, len(chunks), batch_size):
                batch_end = min(batch_idx + batch_size, len(chunks))
                batch_chunks = chunks[batch_idx:batch_end]
                batch_embeddings = embeddings[batch_idx:batch_end]
                
                vectors = []
                for i, (chunk, embedding) in enumerate(zip(batch_chunks, batch_embeddings)):
                    vectors.append({
                        "id": f"chunk_{batch_idx + i}_{hash(chunk) % 1000000}",
                        "values": embedding.tolist(),
                        "metadata": {
                            "text": chunk,
                            "chunk_id": batch_idx + i,
                            "text_length": len(chunk)
                        }
                    })
                
                pinecone_index.upsert(vectors=vectors, namespace=self.namespace)
            
            logger.info(f"Added {len(chunks)} chunks to Pinecone fallback")
        except Exception as e:
            logger.error(f"Failed to add to Pinecone fallback: {e}")

class ImprovedLLMProcessor:
    """Enhanced LLM processor with better prompting and context handling"""
    
    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name
        self.system_prompt = '''You are an expert assistant specializing in legal and constitutional documents, particularly the Indian Constitution.

INSTRUCTIONS:
âš ï¸ Keep each answer concise: **only 1 or 2 sentences per question**.  
âŒ Do not invent any facts.  
âœ… If the context does not answer the question, just think and reason and give the closest answer in max 3 lines. 
! remember your answers will be evaluvated my an AI or any other algorithm, try to get a good score.
!! Do not mention anything like 'the context does not provide specific information about .....' or wnthing like this, just answer the question directly .

REMEMBER : The context always has the answers to the questions. You just have to find it
IMPORTANT: The context contains excerpts from legal documents. Even if the exact phrase isn't found, look for related concepts, principles, or indirect references that can help answer the question.

Respond in valid JSON format:
{
  "answers": [
    "Answer to question 1",
    "Answer to question 2"
  ]
}'''


    def process_large_batch(self, questions: List[str], context_chunks: List[str]) -> List[str]:
        """Process large batches in smaller groups"""
        all_answers = []
        batch_size = 15  # Process 15 questions at a time
        
        for i in range(0, len(questions), batch_size):
            batch_questions = questions[i:i+batch_size]
            
            # Get more targeted context for this batch
            batch_context = self.get_targeted_context(batch_questions, context_chunks)
            
            batch_answers = self.generate_answers(batch_questions, batch_context)
            all_answers.extend(batch_answers)
        
        return all_answers



    def format_context_enhanced(self, chunks: List[str]) -> str:
        """Better context formatting with more structure"""
        if not chunks:
            return "No relevant policy information found."
        
        # Remove duplicates and sort by relevance
        unique_chunks = list(dict.fromkeys(chunks))  # Preserve order, remove dupes
        
        formatted_chunks = []
        for i, chunk in enumerate(unique_chunks[:25]):  # Use more chunks
            clean_chunk = chunk.strip()
            if len(clean_chunk) > 50:  # Only substantial chunks
                formatted_chunks.append(f"[Policy Section {i+1}]\n{clean_chunk}")
        
        return "\n\n".join(formatted_chunks)

    def format_questions(self, questions: List[str]) -> str:
        """Better question formatting"""
        return "\n".join([f"Q{i+1}: {q}" for i, q in enumerate(questions)])


   def generate_answers(self, questions: List[str], context_chunks: List[str]) -> List[str]:
        """FIXED: Process questions in smaller batches with better context"""
        try:
            # PROCESS IN SMALLER BATCHES for better quality
            if len(questions) > 20:
                return self.process_large_batch(questions, context_chunks)
            
            # Prepare better context
            context = self.format_context_enhanced(context_chunks)
            
            # IMPROVED SYSTEM PROMPT
            system_prompt = '''You are an expert insurance policy analyst. 

CRITICAL INSTRUCTIONS:
âš ï¸ ALWAYS cite specific policy clauses, amounts, percentages, and time limits
âš ï¸ Use EXACT terminology from the policy document  
âš ï¸ Each answer should be 2-3 sentences with specific details
âŒ Never give generic answers like "may be covered" - be SPECIFIC
âœ… Reference specific exclusions, waiting periods, sub-limits, and conditions

Example good answer: "Pre-hospitalization expenses are covered up to 30 days before admission as per Section 3.2, subject to 10% of sum insured or Rs. 25,000 whichever is lower."

Respond in valid JSON format with specific, detailed answers.'''

            # Better user message
            user_message = f"""INSURANCE POLICY CONTEXT:
{context}

QUESTIONS TO ANSWER WITH SPECIFIC POLICY DETAILS:
{self.format_questions(questions)}

For each question, provide specific policy references, amounts, time limits, and conditions."""

            response = openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=4000,  # INCREASED for detailed answers
                temperature=0.0,   # REDUCED for more precise answers
                top_p=0.8
            )
            
            response_text = response.choices[0].message.content.strip()
            return self.parse_response(response_text, questions)
            
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            return [f"Unable to process question due to system error." for _ in questions]
    
    def format_context(self, chunks: List[str]) -> str:
        """Format context chunks for better LLM understanding"""
        formatted_chunks = []
        
        for i, chunk in enumerate(chunks):
            # Clean up chunk
            clean_chunk = chunk.strip()
            
            # Add chunk with numbering for reference
            formatted_chunks.append(f"[Chunk {i+1}]\n{clean_chunk}")
        
        return "\n\n".join(formatted_chunks)
    
    def parse_response(self, response_text: str, questions: List[str]) -> List[str]:
        """Parse LLM response with improved error handling"""
        try:
            import json
            import re
            
            # Try to extract JSON
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_match = re.search(r'\{.*?"answers"\s*:\s*\[.*?\].*?\}', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    json_str = response_text
            
            parsed_response = json.loads(json_str)
            
            if "answers" in parsed_response and isinstance(parsed_response["answers"], list):
                answers = parsed_response["answers"]
                
                # Ensure correct number of answers
                while len(answers) < len(questions):
                    answers.append("Unable to find relevant information in the provided context.")
                
                return answers[:len(questions)]
            else:
                raise ValueError("Invalid JSON structure")
                
        except Exception as json_error:
            logger.warning(f"JSON parsing failed: {json_error}")
            logger.warning(f"Raw response: {response_text[:500]}...")
            
            # Fallback parsing
            return self.fallback_parse(response_text, questions)
    
    def fallback_parse(self, response_text: str, questions: List[str]) -> List[str]:
        """Fallback parsing when JSON parsing fails"""
        lines = response_text.split('\n')
        answers = []
        current_answer = ""
        
        for line in lines:
            line = line.strip()
            
            # Skip headers and formatting
            if line.startswith(('```', '{', '}', '"answers"', 'CONTEXT', 'QUESTIONS')):
                continue
            
            # Check if it's a numbered answer
            if re.match(r'^\d+\.', line):
                if current_answer:
                    answers.append(current_answer.strip())
                current_answer = re.sub(r'^\d+\.\s*', '', line)
            elif line and current_answer:
                current_answer += " " + line
            elif line and not current_answer:
                current_answer = line
        
        # Add the last answer
        if current_answer:
            answers.append(current_answer.strip())
        
        # Ensure we have enough answers
        while len(answers) < len(questions):
            answers.append("Unable to process this question due to response parsing issues.")
        
        return answers[:len(questions)]

# Initialize improved processors
pdf_processor = PDFProcessor()
text_chunker = ImprovedTextChunker()
hybrid_vector_store = EnhancedHybridVectorStore()
llm_processor = ImprovedLLMProcessor()

# Create router
router = APIRouter(prefix="/api/v1")

@router.post("/hackrx/run", response_model=ProcessResponse)
async def process_documents(
    request: ProcessRequest,
    credentials: HTTPAuthorizationCredentials = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """FIXED: Better retrieval and processing"""
    try:
        logger.info(f"Processing {len(request.questions)} questions")
        url = str(request.documents)
        
        # Get or process document (same as before)
        cached_document = DatabaseManager.get_document_by_url(db, url)
        if not cached_document:
            text = pdf_processor.download_and_extract_pdf(url)
            chunks = text_chunker.chunk_text(text)
            cached_document = DatabaseManager.save_document(db, url, text, chunks)
            hybrid_vector_store.add_to_pinecone_fallback(chunks)
        
        # IMPROVED: Get MORE targeted chunks per question
        all_relevant_chunks = set()
        
        for question in request.questions[:20]:  # Process first 20 questions more thoroughly
            relevant_chunks = hybrid_vector_store.hybrid_search_enhanced(db, question)
            all_relevant_chunks.update(relevant_chunks[:3])  # Top 3 per question
        
        # For remaining questions, do batch search
        if len(request.questions) > 20:
            remaining_questions = request.questions[20:]
            combined_query = " ".join(remaining_questions[:5])  # Combine queries
            more_chunks = hybrid_vector_store.hybrid_search_enhanced(db, combined_query)
            all_relevant_chunks.update(more_chunks[:10])
        
        # INCREASED final context size
        final_chunks = list(all_relevant_chunks)[:30]  # More context chunks
        
        if not final_chunks:
            answers = ["Relevant policy information not found in document." for _ in request.questions]
        else:
            answers = llm_processor.generate_answers(request.questions, final_chunks)
        
        return ProcessResponse(answers=answers)
        
    except Exception as e:
        logger.error(f"Processing error: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


        
@router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "message": "Enhanced HackRx RAG API is running"}

@router.get("/cache/stats")
async def cache_stats(db: Session = Depends(get_db)):
    """Get cache statistics"""
    try:
        total_docs = db.query(Document).filter(Document.is_active == True).count()
        total_chunks = db.query(DocumentChunk).count()
        
        return {
            "cached_documents": total_docs,
            "total_chunks": total_chunks,
            "cache_status": "active",
            "version": "2.1.0 - Enhanced Retrieval"
        }
    except Exception as e:
        return {"error": str(e)}

# Include router
app.include_router(router)

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "HackRx Enhanced RAG API with Improved Retrieval",
        "version": "2.1.0",
        "improvements": [
            "Better text chunking for legal documents",
            "Enhanced query expansion and search",
            "Improved context formatting for LLM",
            "Multi-query search strategies",
            "Better fallback parsing"
        ],
        "endpoints": {
            "process": "/api/v1/hackrx/run",
            "health": "/api/v1/health",
            "cache_stats": "/api/v1/cache/stats"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
